{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2c5242ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from typing import Dict, List, Tuple\n",
    "\n",
    "from sklearn.preprocessing import KBinsDiscretizer\n",
    "from hmmlearn.hmm import MultinomialHMM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7a16ff0",
   "metadata": {},
   "source": [
    "## Keystroke Dynamics Dataset Schema\n",
    "\n",
    "All timing values are in **seconds** (floating-point). Each row corresponds to one repetition of the password `.tie5Roanl` by a single subject.\n",
    "\n",
    "| Column name       | Type    | Description |\n",
    "|-------------------|---------|-------------|\n",
    "| `subject`         | string  | Unique subject ID (e.g., `s002`, `s057`). Identifies the typist. |\n",
    "| `sessionIndex`    | int     | Session number in which the password was typed (1–8). |\n",
    "| `rep`             | int     | Repetition index of the password within the session (1–50). |\n",
    "\n",
    "### Per-key hold times (`H.*`)\n",
    "\n",
    "| Column name   | Type   | Description |\n",
    "|---------------|--------|-------------|\n",
    "| `H.period`    | float  | Hold time of the `.` key (key down → key up). |\n",
    "| `H.t`         | float  | Hold time of the `t` key. |\n",
    "| `H.i`         | float  | Hold time of the `i` key. |\n",
    "| `H.e`         | float  | Hold time of the `e` key. |\n",
    "| `H.five`      | float  | Hold time of the `5` key. |\n",
    "| `H.Shift.r`   | float  | Hold time of the right Shift key (used to type uppercase `R`). |\n",
    "| `H.o`         | float  | Hold time of the `o` key. |\n",
    "| `H.a`         | float  | Hold time of the `a` key. |\n",
    "| `H.n`         | float  | Hold time of the `n` key. |\n",
    "| `H.l`         | float  | Hold time of the `l` key. |\n",
    "| `H.Return`    | float  | Hold time of the Return/Enter key pressed after the password. |\n",
    "\n",
    "### Keydown–keydown latencies (`DD.*.*`)\n",
    "\n",
    "`DD.key1.key2` = time from **keydown of key1** to **keydown of key2**.\n",
    "\n",
    "| Column name        | Type   | Description |\n",
    "|--------------------|--------|-------------|\n",
    "| `DD.period.t`      | float  | Time from pressing `.` to pressing `t`. |\n",
    "| `DD.t.i`           | float  | Time from pressing `t` to pressing `i`. |\n",
    "| `DD.i.e`           | float  | Time from pressing `i` to pressing `e`. |\n",
    "| `DD.e.five`        | float  | Time from pressing `e` to pressing `5`. |\n",
    "| `DD.five.Shift.r`  | float  | Time from pressing `5` to pressing right Shift. |\n",
    "| `DD.Shift.r.o`     | float  | Time from pressing right Shift to pressing `o`. |\n",
    "| `DD.o.a`           | float  | Time from pressing `o` to pressing `a`. |\n",
    "| `DD.a.n`           | float  | Time from pressing `a` to pressing `n`. |\n",
    "| `DD.n.l`           | float  | Time from pressing `n` to pressing `l`. |\n",
    "| `DD.l.Return`      | float  | Time from pressing `l` to pressing Return. |\n",
    "\n",
    "### Keyup–keydown latencies (`UD.*.*`)\n",
    "\n",
    "`UD.key1.key2` = time from **keyup of key1** to **keydown of key2**.  \n",
    "These values can be **negative** (if key2 is pressed before key1 is released).  \n",
    "Note: `H(key1) + UD(key1,key2) = DD(key1,key2)`.\n",
    "\n",
    "| Column name        | Type   | Description |\n",
    "|--------------------|--------|-------------|\n",
    "| `UD.period.t`      | float  | Time from releasing `.` to pressing `t`. |\n",
    "| `UD.t.i`           | float  | Time from releasing `t` to pressing `i`. |\n",
    "| `UD.i.e`           | float  | Time from releasing `i` to pressing `e`. |\n",
    "| `UD.e.five`        | float  | Time from releasing `e` to pressing `5`. |\n",
    "| `UD.five.Shift.r`  | float  | Time from releasing `5` to pressing right Shift. |\n",
    "| `UD.Shift.r.o`     | float  | Time from releasing right Shift to pressing `o`. |\n",
    "| `UD.o.a`           | float  | Time from releasing `o` to pressing `a`. |\n",
    "| `UD.a.n`           | float  | Time from releasing `a` to pressing `n`. |\n",
    "| `UD.n.l`           | float  | Time from releasing `n` to pressing `l`. |\n",
    "| `UD.l.Return`      | float  | Time from releasing `l` to pressing Return. |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "8bd97a7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "keystroke_data = pd.read_csv(\"data/DSL-StrongPasswordData.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e66b2a56",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subject</th>\n",
       "      <th>sessionIndex</th>\n",
       "      <th>rep</th>\n",
       "      <th>H.period</th>\n",
       "      <th>DD.period.t</th>\n",
       "      <th>UD.period.t</th>\n",
       "      <th>H.t</th>\n",
       "      <th>DD.t.i</th>\n",
       "      <th>UD.t.i</th>\n",
       "      <th>H.i</th>\n",
       "      <th>...</th>\n",
       "      <th>H.a</th>\n",
       "      <th>DD.a.n</th>\n",
       "      <th>UD.a.n</th>\n",
       "      <th>H.n</th>\n",
       "      <th>DD.n.l</th>\n",
       "      <th>UD.n.l</th>\n",
       "      <th>H.l</th>\n",
       "      <th>DD.l.Return</th>\n",
       "      <th>UD.l.Return</th>\n",
       "      <th>H.Return</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>s002</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.1491</td>\n",
       "      <td>0.3979</td>\n",
       "      <td>0.2488</td>\n",
       "      <td>0.1069</td>\n",
       "      <td>0.1674</td>\n",
       "      <td>0.0605</td>\n",
       "      <td>0.1169</td>\n",
       "      <td>...</td>\n",
       "      <td>0.1349</td>\n",
       "      <td>0.1484</td>\n",
       "      <td>0.0135</td>\n",
       "      <td>0.0932</td>\n",
       "      <td>0.3515</td>\n",
       "      <td>0.2583</td>\n",
       "      <td>0.1338</td>\n",
       "      <td>0.3509</td>\n",
       "      <td>0.2171</td>\n",
       "      <td>0.0742</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>s002</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0.1111</td>\n",
       "      <td>0.3451</td>\n",
       "      <td>0.2340</td>\n",
       "      <td>0.0694</td>\n",
       "      <td>0.1283</td>\n",
       "      <td>0.0589</td>\n",
       "      <td>0.0908</td>\n",
       "      <td>...</td>\n",
       "      <td>0.1412</td>\n",
       "      <td>0.2558</td>\n",
       "      <td>0.1146</td>\n",
       "      <td>0.1146</td>\n",
       "      <td>0.2642</td>\n",
       "      <td>0.1496</td>\n",
       "      <td>0.0839</td>\n",
       "      <td>0.2756</td>\n",
       "      <td>0.1917</td>\n",
       "      <td>0.0747</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>s002</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0.1328</td>\n",
       "      <td>0.2072</td>\n",
       "      <td>0.0744</td>\n",
       "      <td>0.0731</td>\n",
       "      <td>0.1291</td>\n",
       "      <td>0.0560</td>\n",
       "      <td>0.0821</td>\n",
       "      <td>...</td>\n",
       "      <td>0.1621</td>\n",
       "      <td>0.2332</td>\n",
       "      <td>0.0711</td>\n",
       "      <td>0.1172</td>\n",
       "      <td>0.2705</td>\n",
       "      <td>0.1533</td>\n",
       "      <td>0.1085</td>\n",
       "      <td>0.2847</td>\n",
       "      <td>0.1762</td>\n",
       "      <td>0.0945</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>s002</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>0.1291</td>\n",
       "      <td>0.2515</td>\n",
       "      <td>0.1224</td>\n",
       "      <td>0.1059</td>\n",
       "      <td>0.2495</td>\n",
       "      <td>0.1436</td>\n",
       "      <td>0.1040</td>\n",
       "      <td>...</td>\n",
       "      <td>0.1457</td>\n",
       "      <td>0.1629</td>\n",
       "      <td>0.0172</td>\n",
       "      <td>0.0866</td>\n",
       "      <td>0.2341</td>\n",
       "      <td>0.1475</td>\n",
       "      <td>0.0845</td>\n",
       "      <td>0.3232</td>\n",
       "      <td>0.2387</td>\n",
       "      <td>0.0813</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>s002</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>0.1249</td>\n",
       "      <td>0.2317</td>\n",
       "      <td>0.1068</td>\n",
       "      <td>0.0895</td>\n",
       "      <td>0.1676</td>\n",
       "      <td>0.0781</td>\n",
       "      <td>0.0903</td>\n",
       "      <td>...</td>\n",
       "      <td>0.1312</td>\n",
       "      <td>0.1582</td>\n",
       "      <td>0.0270</td>\n",
       "      <td>0.0884</td>\n",
       "      <td>0.2517</td>\n",
       "      <td>0.1633</td>\n",
       "      <td>0.0903</td>\n",
       "      <td>0.2517</td>\n",
       "      <td>0.1614</td>\n",
       "      <td>0.0818</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 34 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  subject  sessionIndex  rep  H.period  DD.period.t  UD.period.t     H.t  \\\n",
       "0    s002             1    1    0.1491       0.3979       0.2488  0.1069   \n",
       "1    s002             1    2    0.1111       0.3451       0.2340  0.0694   \n",
       "2    s002             1    3    0.1328       0.2072       0.0744  0.0731   \n",
       "3    s002             1    4    0.1291       0.2515       0.1224  0.1059   \n",
       "4    s002             1    5    0.1249       0.2317       0.1068  0.0895   \n",
       "\n",
       "   DD.t.i  UD.t.i     H.i  ...     H.a  DD.a.n  UD.a.n     H.n  DD.n.l  \\\n",
       "0  0.1674  0.0605  0.1169  ...  0.1349  0.1484  0.0135  0.0932  0.3515   \n",
       "1  0.1283  0.0589  0.0908  ...  0.1412  0.2558  0.1146  0.1146  0.2642   \n",
       "2  0.1291  0.0560  0.0821  ...  0.1621  0.2332  0.0711  0.1172  0.2705   \n",
       "3  0.2495  0.1436  0.1040  ...  0.1457  0.1629  0.0172  0.0866  0.2341   \n",
       "4  0.1676  0.0781  0.0903  ...  0.1312  0.1582  0.0270  0.0884  0.2517   \n",
       "\n",
       "   UD.n.l     H.l  DD.l.Return  UD.l.Return  H.Return  \n",
       "0  0.2583  0.1338       0.3509       0.2171    0.0742  \n",
       "1  0.1496  0.0839       0.2756       0.1917    0.0747  \n",
       "2  0.1533  0.1085       0.2847       0.1762    0.0945  \n",
       "3  0.1475  0.0845       0.3232       0.2387    0.0813  \n",
       "4  0.1633  0.0903       0.2517       0.1614    0.0818  \n",
       "\n",
       "[5 rows x 34 columns]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keystroke_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6c01f93",
   "metadata": {},
   "source": [
    "# Hidden Markov Model for Keystroke Dynamics Authentication\n",
    "\n",
    "This section explains, in mathematical and intuitive terms, how the Hidden Markov Model (HMM) we built works for keystroke-based user authentication. The notation matches the standard HMM notation:\n",
    "\n",
    "- Hidden states: $S_t$\n",
    "- Observations: $O_t$\n",
    "- Initial distribution: $\\pi$\n",
    "- Transition matrix: $A$\n",
    "- Emission matrix: $B$\n",
    "- Forward variables: $\\alpha$\n",
    "- Backward variables: $\\beta$\n",
    "- Posteriors: $\\gamma, \\xi$\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac73624e",
   "metadata": {},
   "source": [
    "## 1. From Keystrokes to Observation Sequences\n",
    "\n",
    "Firstly, we need to discretize our data into a form that is easily digestable for an HMM.\n",
    "\n",
    "This is the current state of each observation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "3b751639",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>subject</th>\n",
       "      <td>s002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sessionIndex</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rep</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>H.period</th>\n",
       "      <td>0.1491</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DD.period.t</th>\n",
       "      <td>0.3979</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>UD.period.t</th>\n",
       "      <td>0.2488</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>H.t</th>\n",
       "      <td>0.1069</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DD.t.i</th>\n",
       "      <td>0.1674</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>UD.t.i</th>\n",
       "      <td>0.0605</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>H.i</th>\n",
       "      <td>0.1169</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DD.i.e</th>\n",
       "      <td>0.2212</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>UD.i.e</th>\n",
       "      <td>0.1043</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>H.e</th>\n",
       "      <td>0.1417</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DD.e.five</th>\n",
       "      <td>1.1885</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>UD.e.five</th>\n",
       "      <td>1.0468</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>H.five</th>\n",
       "      <td>0.1146</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DD.five.Shift.r</th>\n",
       "      <td>1.6055</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>UD.five.Shift.r</th>\n",
       "      <td>1.4909</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>H.Shift.r</th>\n",
       "      <td>0.1067</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DD.Shift.r.o</th>\n",
       "      <td>0.759</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>UD.Shift.r.o</th>\n",
       "      <td>0.6523</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>H.o</th>\n",
       "      <td>0.1016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DD.o.a</th>\n",
       "      <td>0.2136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>UD.o.a</th>\n",
       "      <td>0.112</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>H.a</th>\n",
       "      <td>0.1349</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DD.a.n</th>\n",
       "      <td>0.1484</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>UD.a.n</th>\n",
       "      <td>0.0135</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>H.n</th>\n",
       "      <td>0.0932</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DD.n.l</th>\n",
       "      <td>0.3515</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>UD.n.l</th>\n",
       "      <td>0.2583</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>H.l</th>\n",
       "      <td>0.1338</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DD.l.Return</th>\n",
       "      <td>0.3509</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>UD.l.Return</th>\n",
       "      <td>0.2171</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>H.Return</th>\n",
       "      <td>0.0742</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      0\n",
       "subject            s002\n",
       "sessionIndex          1\n",
       "rep                   1\n",
       "H.period         0.1491\n",
       "DD.period.t      0.3979\n",
       "UD.period.t      0.2488\n",
       "H.t              0.1069\n",
       "DD.t.i           0.1674\n",
       "UD.t.i           0.0605\n",
       "H.i              0.1169\n",
       "DD.i.e           0.2212\n",
       "UD.i.e           0.1043\n",
       "H.e              0.1417\n",
       "DD.e.five        1.1885\n",
       "UD.e.five        1.0468\n",
       "H.five           0.1146\n",
       "DD.five.Shift.r  1.6055\n",
       "UD.five.Shift.r  1.4909\n",
       "H.Shift.r        0.1067\n",
       "DD.Shift.r.o      0.759\n",
       "UD.Shift.r.o     0.6523\n",
       "H.o              0.1016\n",
       "DD.o.a           0.2136\n",
       "UD.o.a            0.112\n",
       "H.a              0.1349\n",
       "DD.a.n           0.1484\n",
       "UD.a.n           0.0135\n",
       "H.n              0.0932\n",
       "DD.n.l           0.3515\n",
       "UD.n.l           0.2583\n",
       "H.l              0.1338\n",
       "DD.l.Return      0.3509\n",
       "UD.l.Return      0.2171\n",
       "H.Return         0.0742"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(keystroke_data.iloc[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df5281cd",
   "metadata": {},
   "source": [
    "Each row in the dataset corresponds to **one repetition of the same password** (e.g. `.tie5Roanl`) typed by some subject.\n",
    "\n",
    "The observations look like:\n",
    "\n",
    "- Metadata: `subject`, `sessionIndex`, `rep`\n",
    "- Timing features (31 columns):\n",
    "  - Hold times: `H.period`, `H.t`, …, `H.Return`\n",
    "  - Keydown–keydown times: `DD.period.t`, `DD.t.i`, …, `DD.l.Return`\n",
    "  - Keyup–keydown times: `UD.period.t`, `UD.t.i`, …, `UD.l.Return`\n",
    "\n",
    "So for one repetition, we have a vector of 31 timing values:\n",
    "\n",
    "$$\n",
    "\\mathbf{x} = (x_1, x_2, \\dots, x_T), \\quad T \\approx 31.\n",
    "$$\n",
    "\n",
    "We then **discretize** each continuous timing value into a small number of bins:\n",
    "\n",
    "- Collect all timing values across the dataset.\n",
    "- Use quantile-based binning into $m$ bins (e.g. $m = 10$).\n",
    "- This means that the bins are constructed such that roughly $\\frac{1}{m}$ of all observed timings in the dataset fall within each bin.\n",
    "- For each feature $x_t$, map it to a bin index:\n",
    "  $$\n",
    "  O_t \\in \\{0, 1, \\dots, m-1\\}.\n",
    "  $$\n",
    "\n",
    "So **one password repetition** becomes a sequence of discrete observations:\n",
    "\n",
    "$$\n",
    "\\mathbf{o} = (o_1, o_2, \\dots, o_T), \\quad o_t \\in \\{0,\\dots,m-1\\}.\n",
    "$$\n",
    "\n",
    "Each user has many such sequences (up to ~400 repetitions).\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "6359a0ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_sequences_discrete(\n",
    "    df: pd.DataFrame,\n",
    "    n_bins: int = 10,\n",
    "    min_reps_per_subject: int = 20,\n",
    ") -> Tuple[Dict[str, List[np.ndarray]], int, KBinsDiscretizer]:\n",
    "    \"\"\"\n",
    "    Build per-subject sequences of discrete symbols for each repetition.\n",
    "\n",
    "    Each row = one password repetition for some subject.\n",
    "    For that row:\n",
    "      - We have 31 timing features (H.*, DD.*, UD.*, ...).\n",
    "      - Each timing value is discretized via global quantile bins into {0,...,n_bins-1}.\n",
    "      - The sequence is the length-31 vector of symbols for that repetition.\n",
    "\n",
    "    Returns:\n",
    "        subject_to_sequences: dict[subject -> list of np.array(T,) of ints], T = #timing cols\n",
    "        n_obs: number of discrete symbols (n_bins)\n",
    "        discretizer: fitted KBinsDiscretizer (for consistency if you transform new data)\n",
    "    \"\"\"\n",
    "    meta_cols = [\"subject\", \"sessionIndex\", \"rep\"]\n",
    "    feature_cols = [c for c in df.columns if c not in meta_cols]\n",
    "\n",
    "    # Flatten all timing values into a single vector for global quantile binning\n",
    "    all_values = df[feature_cols].to_numpy(dtype=float).reshape(-1, 1)\n",
    "\n",
    "    discretizer = KBinsDiscretizer(\n",
    "        n_bins=n_bins,\n",
    "        encode=\"ordinal\",\n",
    "        strategy=\"quantile\",\n",
    "    )\n",
    "    discretizer.fit(all_values)\n",
    "\n",
    "    subject_to_sequences: Dict[str, List[np.ndarray]] = {}\n",
    "\n",
    "    for subj, df_subj in df.groupby(\"subject\"):\n",
    "        df_subj = df_subj.sort_values(by=[\"sessionIndex\", \"rep\"])\n",
    "        if len(df_subj) < min_reps_per_subject:\n",
    "            continue\n",
    "\n",
    "        seqs: List[np.ndarray] = []\n",
    "        for _, row in df_subj.iterrows():\n",
    "            vals = row[feature_cols].to_numpy(dtype=float).reshape(-1, 1)  # (31,1)\n",
    "            symbols = discretizer.transform(vals).astype(int).ravel()      # (31,)\n",
    "            seqs.append(symbols)\n",
    "\n",
    "        if seqs:\n",
    "            subject_to_sequences[subj] = seqs\n",
    "\n",
    "        \n",
    "    \n",
    "    # return the sequences, the number of bins, and the discretizer for consistency during prediction.\n",
    "    return subject_to_sequences, n_bins, discretizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "7fe2452f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/envs/datasci-sec/lib/python3.11/site-packages/sklearn/preprocessing/_discretization.py:296: FutureWarning: The current default behavior, quantile_method='linear', will be changed to quantile_method='averaged_inverted_cdf' in scikit-learn version 1.9 to naturally support sample weight equivalence properties by default. Pass quantile_method='averaged_inverted_cdf' explicitly to silence this warning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>390</th>\n",
       "      <th>391</th>\n",
       "      <th>392</th>\n",
       "      <th>393</th>\n",
       "      <th>394</th>\n",
       "      <th>395</th>\n",
       "      <th>396</th>\n",
       "      <th>397</th>\n",
       "      <th>398</th>\n",
       "      <th>399</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>s002</th>\n",
       "      <td>[6, 9, 8, 4, 7, 1, 5, 7, 4, 6, 9, 9, 5, 9, 9, ...</td>\n",
       "      <td>[5, 9, 8, 2, 5, 1, 3, 6, 1, 3, 9, 9, 2, 9, 9, ...</td>\n",
       "      <td>[6, 7, 2, 2, 5, 1, 3, 6, 2, 3, 9, 9, 3, 9, 9, ...</td>\n",
       "      <td>[5, 8, 5, 4, 8, 6, 4, 7, 4, 3, 9, 9, 3, 9, 9, ...</td>\n",
       "      <td>[5, 8, 4, 3, 7, 2, 3, 6, 2, 3, 9, 9, 2, 9, 9, ...</td>\n",
       "      <td>[6, 8, 4, 3, 5, 1, 2, 6, 2, 3, 9, 9, 4, 9, 9, ...</td>\n",
       "      <td>[4, 7, 4, 3, 6, 1, 3, 6, 1, 2, 9, 9, 3, 9, 9, ...</td>\n",
       "      <td>[3, 7, 3, 3, 6, 1, 2, 6, 1, 2, 9, 9, 6, 9, 9, ...</td>\n",
       "      <td>[4, 7, 3, 2, 5, 1, 3, 6, 1, 2, 9, 9, 3, 9, 9, ...</td>\n",
       "      <td>[4, 7, 2, 2, 6, 2, 2, 5, 1, 3, 9, 9, 3, 9, 9, ...</td>\n",
       "      <td>...</td>\n",
       "      <td>[6, 7, 1, 5, 7, 1, 5, 4, 0, 5, 8, 7, 6, 9, 9, ...</td>\n",
       "      <td>[6, 6, 0, 5, 6, 0, 5, 5, 0, 5, 8, 6, 6, 9, 9, ...</td>\n",
       "      <td>[6, 8, 3, 5, 9, 8, 6, 7, 1, 6, 8, 7, 6, 9, 9, ...</td>\n",
       "      <td>[6, 7, 1, 5, 7, 1, 5, 6, 0, 6, 8, 4, 6, 9, 9, ...</td>\n",
       "      <td>[6, 7, 1, 5, 5, 0, 6, 7, 1, 6, 8, 5, 7, 9, 8, ...</td>\n",
       "      <td>[6, 7, 1, 1, 5, 1, 4, 5, 0, 3, 8, 6, 4, 9, 9, ...</td>\n",
       "      <td>[7, 8, 1, 5, 7, 2, 4, 6, 0, 4, 9, 9, 5, 9, 9, ...</td>\n",
       "      <td>[6, 7, 0, 3, 5, 0, 5, 6, 0, 3, 8, 6, 6, 9, 9, ...</td>\n",
       "      <td>[6, 7, 1, 3, 7, 5, 6, 7, 2, 5, 9, 8, 6, 9, 9, ...</td>\n",
       "      <td>[7, 7, 0, 5, 6, 1, 6, 6, 0, 4, 9, 9, 4, 9, 9, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows × 400 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    0    \\\n",
       "s002  [6, 9, 8, 4, 7, 1, 5, 7, 4, 6, 9, 9, 5, 9, 9, ...   \n",
       "\n",
       "                                                    1    \\\n",
       "s002  [5, 9, 8, 2, 5, 1, 3, 6, 1, 3, 9, 9, 2, 9, 9, ...   \n",
       "\n",
       "                                                    2    \\\n",
       "s002  [6, 7, 2, 2, 5, 1, 3, 6, 2, 3, 9, 9, 3, 9, 9, ...   \n",
       "\n",
       "                                                    3    \\\n",
       "s002  [5, 8, 5, 4, 8, 6, 4, 7, 4, 3, 9, 9, 3, 9, 9, ...   \n",
       "\n",
       "                                                    4    \\\n",
       "s002  [5, 8, 4, 3, 7, 2, 3, 6, 2, 3, 9, 9, 2, 9, 9, ...   \n",
       "\n",
       "                                                    5    \\\n",
       "s002  [6, 8, 4, 3, 5, 1, 2, 6, 2, 3, 9, 9, 4, 9, 9, ...   \n",
       "\n",
       "                                                    6    \\\n",
       "s002  [4, 7, 4, 3, 6, 1, 3, 6, 1, 2, 9, 9, 3, 9, 9, ...   \n",
       "\n",
       "                                                    7    \\\n",
       "s002  [3, 7, 3, 3, 6, 1, 2, 6, 1, 2, 9, 9, 6, 9, 9, ...   \n",
       "\n",
       "                                                    8    \\\n",
       "s002  [4, 7, 3, 2, 5, 1, 3, 6, 1, 2, 9, 9, 3, 9, 9, ...   \n",
       "\n",
       "                                                    9    ...  \\\n",
       "s002  [4, 7, 2, 2, 6, 2, 2, 5, 1, 3, 9, 9, 3, 9, 9, ...  ...   \n",
       "\n",
       "                                                    390  \\\n",
       "s002  [6, 7, 1, 5, 7, 1, 5, 4, 0, 5, 8, 7, 6, 9, 9, ...   \n",
       "\n",
       "                                                    391  \\\n",
       "s002  [6, 6, 0, 5, 6, 0, 5, 5, 0, 5, 8, 6, 6, 9, 9, ...   \n",
       "\n",
       "                                                    392  \\\n",
       "s002  [6, 8, 3, 5, 9, 8, 6, 7, 1, 6, 8, 7, 6, 9, 9, ...   \n",
       "\n",
       "                                                    393  \\\n",
       "s002  [6, 7, 1, 5, 7, 1, 5, 6, 0, 6, 8, 4, 6, 9, 9, ...   \n",
       "\n",
       "                                                    394  \\\n",
       "s002  [6, 7, 1, 5, 5, 0, 6, 7, 1, 6, 8, 5, 7, 9, 8, ...   \n",
       "\n",
       "                                                    395  \\\n",
       "s002  [6, 7, 1, 1, 5, 1, 4, 5, 0, 3, 8, 6, 4, 9, 9, ...   \n",
       "\n",
       "                                                    396  \\\n",
       "s002  [7, 8, 1, 5, 7, 2, 4, 6, 0, 4, 9, 9, 5, 9, 9, ...   \n",
       "\n",
       "                                                    397  \\\n",
       "s002  [6, 7, 0, 3, 5, 0, 5, 6, 0, 3, 8, 6, 6, 9, 9, ...   \n",
       "\n",
       "                                                    398  \\\n",
       "s002  [6, 7, 1, 3, 7, 5, 6, 7, 2, 5, 9, 8, 6, 9, 9, ...   \n",
       "\n",
       "                                                    399  \n",
       "s002  [7, 7, 0, 5, 6, 1, 6, 6, 0, 4, 9, 9, 4, 9, 9, ...  \n",
       "\n",
       "[1 rows x 400 columns]"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subject002 = pd.DataFrame(build_sequences_discrete(keystroke_data, n_bins=10, min_reps_per_subject=20)[0]).T.iloc[[0]]\n",
    "subject002"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a49f6887",
   "metadata": {},
   "source": [
    "Now, for each user, we have discretized the timing of their keystrokes into bins."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64fa1f82",
   "metadata": {},
   "source": [
    "A full example of how the observations from one password entry will be broken down for the HMM:\n",
    "\n",
    "For a single time step $t$, if we use $n_{\\text{bins}} = 10$ and the discretized bin is $o_t = 3$, then the **one-hot binned observation vector** $\\mathbf{x}_t$ looks like:\n",
    "\n",
    "$$\n",
    "\\mathbf{x}_t = [\\,0,\\ 0,\\ 0,\\ 1,\\ 0,\\ 0,\\ 0,\\ 0,\\ 0,\\ 0\\,].\n",
    "$$\n",
    "\n",
    "Index-by-index (0-based):\n",
    "\n",
    "- bin 0 $\\rightarrow$ 0  \n",
    "- bin 1 $\\rightarrow$ 0  \n",
    "- bin 2 $\\rightarrow$ 0  \n",
    "- bin 3 $\\rightarrow$ 1  (this matches $o_t = 3$)  \n",
    "- bin 4 $\\rightarrow$ 0  \n",
    "- bin 5 $\\rightarrow$ 0  \n",
    "- bin 6 $\\rightarrow$ 0  \n",
    "- bin 7 $\\rightarrow$ 0  \n",
    "- bin 8 $\\rightarrow$ 0  \n",
    "- bin 9 $\\rightarrow$ 0  \n",
    "\n",
    "For a full password repetition of length $T \\approx 31$, suppose the sequence of bins is\n",
    "\n",
    "$$\n",
    "\\mathbf{o} = [\\,3,\\ 5,\\ 1,\\ 7,\\ 2,\\ \\dots\\,].\n",
    "$$\n",
    "\n",
    "The corresponding one-hot matrix $X$ (what the HMM actually sees) is:\n",
    "\n",
    "$$\n",
    "X =\n",
    "\\begin{bmatrix}\n",
    "0 & 0 & 0 & 1 & 0 & 0 & 0 & 0 & 0 & 0 \\\\\\\\\n",
    "0 & 0 & 0 & 0 & 0 & 1 & 0 & 0 & 0 & 0 \\\\\\\\\n",
    "0 & 1 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\\\\\\\\n",
    "0 & 0 & 0 & 0 & 0 & 0 & 0 & 1 & 0 & 0 \\\\\\\\\n",
    "0 & 0 & 1 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\\\\\\\\n",
    "\\vdots & \\vdots & \\vdots & \\vdots & \\vdots & \\vdots & \\vdots & \\vdots & \\vdots & \\vdots\n",
    "\\end{bmatrix}.\n",
    "$$\n",
    "\n",
    "Each row of $X$ is the one-hot vector $\\mathbf{x}_t$ for one time step $t$, and each column corresponds to one timing bin (0 through 9).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "152d699c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sequences_to_one_hot(\n",
    "    seqs: List[np.ndarray],\n",
    "    n_obs: int,\n",
    ") -> Tuple[np.ndarray, List[int]]:\n",
    "    \"\"\"\n",
    "    Convert a list of integer sequences into a one-hot matrix + lengths.\n",
    "\n",
    "    seqs: list of arrays of shape (T,) with integers in [0, n_obs-1]\n",
    "    Returns:\n",
    "        X: (sum_T, n_obs) one-hot, dtype=int\n",
    "        lengths: list of T for each sequence\n",
    "    \"\"\"\n",
    "    eye = np.eye(n_obs, dtype=int)\n",
    "    one_hot_blocks = []\n",
    "    lengths = []\n",
    "\n",
    "    for s in seqs:\n",
    "        s = np.asarray(s, dtype=int)\n",
    "        one_hot = eye[s]             # (T, n_obs), still int\n",
    "        one_hot_blocks.append(one_hot)\n",
    "        lengths.append(len(s))\n",
    "\n",
    "    X = np.concatenate(one_hot_blocks, axis=0)  # (sum_T, n_obs), int\n",
    "    return X, lengths"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb3478b8",
   "metadata": {},
   "source": [
    "One hot encoding example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "03529210",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 1],\n",
       "       [0, 0, 0, ..., 0, 1, 0],\n",
       "       ...,\n",
       "       [0, 0, 0, ..., 0, 0, 1],\n",
       "       [0, 0, 0, ..., 0, 0, 1],\n",
       "       [0, 0, 0, ..., 0, 0, 0]])"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subject002_seqs = list(subject002.values.flatten())\n",
    "subject002_one_hot = sequences_to_one_hot(subject002_seqs, n_obs=10)\n",
    "subject002_one_hot[0]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f2f2e42",
   "metadata": {},
   "source": [
    "Now that we have our data preprocessed and ready for ML, we can focus on the HMM."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "500afbd1",
   "metadata": {},
   "source": [
    "## 2. HMM for a Single User\n",
    "\n",
    "For each user $u$, we train a **separate HMM**:\n",
    "\n",
    "- Hidden state at time $t$: $S_t \\in \\{1,\\dots,n\\}$\n",
    "  - Interpret these as latent **typing modes** for that user (e.g. “steady”, “rushing”, “hesitating”).\n",
    "- Observation at time $t$: $O_t \\in \\{0,\\dots,m-1\\}$\n",
    "  - The discretized timing bin for feature $t$.\n",
    "\n",
    "The HMM parameters for user $u$ (we drop the superscript $(u)$ for readability) are:\n",
    "\n",
    "- Initial state distribution\n",
    "  $$\n",
    "  \\pi_i = P(S_1 = i), \\quad i = 1,\\dots,n.\n",
    "  $$\n",
    "- Transition probabilities\n",
    "  $$\n",
    "  a_{ij} = P(S_{t+1} = j \\mid S_t = i), \\quad i,j = 1,\\dots,n.\n",
    "  $$\n",
    "- Emission probabilities\n",
    "  $$\n",
    "  b_{ik} = P(O_t = k \\mid S_t = i), \\quad i = 1,\\dots,n,\\; k = 0,\\dots,m-1.\n",
    "  $$\n",
    "\n",
    "We write these as:\n",
    "\n",
    "- $\\boldsymbol{\\pi} = (\\pi_1,\\dots,\\pi_n)$\n",
    "- $A = [a_{ij}]_{i,j=1}^n$\n",
    "- $B = [b_{ik}]_{\\substack{i=1,\\dots,n \\\\ k=0,\\dots,m-1}}$\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bced3ac4",
   "metadata": {},
   "source": [
    "\n",
    "## 3. Generative Story and Joint Probability\n",
    "\n",
    "The HMM defines a **generative process** for a sequence:\n",
    "\n",
    "1. Sample initial hidden state:\n",
    "   $$\n",
    "   S_1 \\sim (\\pi_1,\\dots,\\pi_n).\n",
    "   $$\n",
    "\n",
    "2. Emit first observation:\n",
    "   $$\n",
    "   O_1 \\sim (b_{1,o_1}, \\dots, b_{n,o_1}) \\text{ conditioned on } S_1.\n",
    "   $$\n",
    "\n",
    "3. For each time step $t = 1,\\dots,T-1$:\n",
    "   - Transition:\n",
    "     $$\n",
    "     S_{t+1} \\sim (a_{S_t,1},\\dots,a_{S_t,n}).\n",
    "     $$\n",
    "   - Emission:\n",
    "     $$\n",
    "     O_{t+1} \\sim (b_{S_{t+1},0},\\dots,b_{S_{t+1},m-1}).\n",
    "     $$\n",
    "\n",
    "Under the Markov and conditional-independence assumptions, the **joint probability** of a state sequence $\\mathbf{s} = (s_1,\\dots,s_T)$ and an observation sequence $\\mathbf{o} = (o_1,\\dots,o_T)$ is:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "P(\\mathbf{s}, \\mathbf{o})\n",
    "&= P(S_1 = s_1)\\,P(O_1 = o_1 \\mid S_1 = s_1)\\,\\prod_{t=1}^{T-1} P(S_{t+1} = s_{t+1} \\mid S_t = s_t)\\,P(O_{t+1} = o_{t+1} \\mid S_{t+1} = s_{t+1}) \\\\\n",
    "&= \\pi_{s_1} \\, b_{s_1,o_1} \\, \\prod_{t=1}^{T-1} a_{s_t,s_{t+1}}\\, b_{s_{t+1},o_{t+1}}.\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "The **likelihood** of the observed sequence under the model is obtained by summing over all hidden paths:\n",
    "\n",
    "$$\n",
    "P(\\mathbf{o}) = \\sum_{s_1,\\dots,s_T} P(\\mathbf{s}, \\mathbf{o}).\n",
    "$$\n",
    "\n",
    "We will make use of maximizing this likelihood later.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75b0d4e5",
   "metadata": {},
   "source": [
    "## 4. Discrete Multinomial HMM\n",
    "\n",
    "The new `MultinomialHMM` interprets observations at each time step as **count vectors**:\n",
    "\n",
    "- For $m$ possible symbols, an observation at time $t$ is a vector:\n",
    "  $$\n",
    "  \\mathbf{x}_t \\in \\mathbb{N}^m,\\quad \\sum_{k=1}^m x_{t,k} = \\text{n\\_trials}.\n",
    "  $$\n",
    "\n",
    "We want a **categorical** distribution over symbols $\\{0,\\dots,m-1\\}$. We achieve this by:\n",
    "\n",
    "- Setting `n_trials = 1`.\n",
    "- Representing a symbol $o_t$ as a **one-hot vector**:\n",
    "  $$\n",
    "  \\mathbf{x}_t = \\text{one\\_hot}(o_t) \\in \\{0,1\\}^m,\n",
    "  \\quad \\sum_k x_{t,k} = 1.\n",
    "  $$\n",
    "\n",
    "For hidden state $i$, the emission parameters are:\n",
    "\n",
    "$$\n",
    "\\mathbf{b}_i = (b_{i0}, b_{i1}, \\dots, b_{i,m-1}),\n",
    "\\quad \\sum_{k=0}^{m-1} b_{ik} = 1.\n",
    "$$\n",
    "\n",
    "Then the emission probability is:\n",
    "\n",
    "$$\n",
    "P(\\mathbf{x}_t \\mid S_t = i)\n",
    "= \\text{Multinomial}(\\mathbf{x}_t; \\text{n\\_trials}=1, \\mathbf{b}_i)\n",
    "= b_{i, o_t}.\n",
    "$$\n",
    "\n",
    "So the `MultinomialHMM` with one-hot counts and `n_trials = 1` is exactly a **categorical observation model**.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3f11417",
   "metadata": {},
   "source": [
    "## 5. Forward Algorithm: Computing $P(\\mathbf{o})$\n",
    "\n",
    "Define the **forward variables**:\n",
    "\n",
    "$$\n",
    "\\alpha_{i t} = P(o_1, \\dots, o_t, S_t = i), \\quad i = 1,\\dots,n,\\; t = 1,\\dots,T.\n",
    "$$\n",
    "\n",
    "Intuition: $\\alpha_{i t}$ is the probability that after observing the first $t$ timing bins, the HMM is in hidden state $i$ and has produced exactly those observations.\n",
    "\n",
    "### Initialization ($t = 1$)\n",
    "\n",
    "$$\n",
    "\\alpha_{i 1}\n",
    "= P(S_1 = i, o_1)\n",
    "= \\pi_i \\, b_{i, o_1}.\n",
    "$$\n",
    "\n",
    "### Recurrence ($t = 1,\\dots,T-1$)\n",
    "\n",
    "$$\n",
    "\\alpha_{j, t+1}\n",
    "= P(o_1,\\dots,o_{t+1}, S_{t+1} = j)\n",
    "= \\sum_{i=1}^n \\alpha_{i t} \\, a_{i j} \\, b_{j, o_{t+1}}.\n",
    "$$\n",
    "\n",
    "This is a dynamic program: each column of the $\\alpha$-table is computed from the previous one.\n",
    "\n",
    "### Likelihood\n",
    "\n",
    "Once we reach time $T$:\n",
    "\n",
    "$$\n",
    "P(\\mathbf{o})\n",
    "= P(o_1,\\dots,o_T)\n",
    "= \\sum_{i=1}^n \\alpha_{i T}.\n",
    "$$\n",
    "\n",
    "In practice, we work in **log space** and use scaling to avoid underflow. The library’s `score()` function is computing (a scaled version of) this log-likelihood.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "478b5d36",
   "metadata": {},
   "source": [
    "## 6. Backward Algorithm and Posteriors\n",
    "\n",
    "For EM learning, we also need the **backward variables**:\n",
    "\n",
    "$$\n",
    "\\beta_{i t} = P(o_{t+1}, \\dots, o_T \\mid S_t = i).\n",
    "$$\n",
    "\n",
    "Intuition: $\\beta_{i t}$ is the probability of generating the tail of the sequence starting from state $i$ at time $t$.\n",
    "\n",
    "### Initialization ($t = T$)\n",
    "\n",
    "$$\n",
    "\\beta_{i T} = 1, \\quad \\forall i.\n",
    "$$\n",
    "\n",
    "### Recurrence ($t = T-1,\\dots,1$)\n",
    "\n",
    "$$\n",
    "\\beta_{i t}\n",
    "= \\sum_{j=1}^n a_{i j} \\, b_{j, o_{t+1}} \\, \\beta_{j, t+1}.\n",
    "$$\n",
    "\n",
    "Given both $\\alpha$ and $\\beta$, we can compute **posterior probabilities** over states and transitions.\n",
    "\n",
    "#### State posterior ($\\gamma$)\n",
    "\n",
    "$$\n",
    "\\gamma_{i t}\n",
    "= P(S_t = i \\mid \\mathbf{o})\n",
    "= \\frac{\\alpha_{i t}\\,\\beta_{i t}}{\\sum_{k=1}^n \\alpha_{k t}\\,\\beta_{k t}}.\n",
    "$$\n",
    "\n",
    "Interpretation: probability that we were in mode $i$ at time $t$ given the whole sequence.\n",
    "\n",
    "#### Transition posterior ($\\xi$)\n",
    "\n",
    "$$\n",
    "\\xi_{i j, t}\n",
    "= P(S_t = i, S_{t+1} = j \\mid \\mathbf{o})\n",
    "= \\frac{\\alpha_{i t}\\, a_{i j} \\, b_{j, o_{t+1}} \\, \\beta_{j, t+1}}\n",
    "       {\\sum_{k=1}^n \\sum_{\\ell=1}^n \\alpha_{k t}\\, a_{k \\ell} \\, b_{\\ell, o_{t+1}} \\, \\beta_{\\ell, t+1}}.\n",
    "$$\n",
    "\n",
    "These $\\gamma_{i t}$ and $\\xi_{i j, t}$ are the “soft counts” of how often each state and transition is used, conditioned on the data.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae22b9bc",
   "metadata": {},
   "source": [
    "## 7. EM Learning for One User\n",
    "\n",
    "For a single user, we have many repetition sequences:\n",
    "\n",
    "$$\n",
    "\\{\\mathbf{o}^{(r)}\\}_{r=1}^R, \\quad \\mathbf{o}^{(r)} = (o^{(r)}_1,\\dots,o^{(r)}_{T_r}).\n",
    "$$\n",
    "\n",
    "We want to find parameters $\\theta = (\\boldsymbol{\\pi}, A, B)$ that **maximize the likelihood**:\n",
    "\n",
    "$$\n",
    "\\theta^* = \\arg\\max_\\theta \\sum_{r=1}^R \\log P(\\mathbf{o}^{(r)} \\mid \\theta).\n",
    "$$\n",
    "\n",
    "This is done using **EM**, known as the **Baum–Welch algorithm** for HMMs.\n",
    "\n",
    "### E-step\n",
    "\n",
    "For each sequence $\\mathbf{o}^{(r)}$:\n",
    "\n",
    "1. Run forward–backward to get $\\alpha^{(r)}_{i t}$, $\\beta^{(r)}_{i t}$.\n",
    "2. Compute:\n",
    "   - $\\gamma^{(r)}_{i t} = P(S^{(r)}_t = i \\mid \\mathbf{o}^{(r)})$.\n",
    "   - $\\xi^{(r)}_{i j, t} = P(S^{(r)}_t = i, S^{(r)}_{t+1} = j \\mid \\mathbf{o}^{(r)})$.\n",
    "\n",
    "### M-step\n",
    "\n",
    "Update parameters by treating these posteriors as expected counts.\n",
    "\n",
    "1. **Initial distribution**:\n",
    "   $$\n",
    "   \\pi_i \\leftarrow\n",
    "   \\frac{1}{R} \\sum_{r=1}^R P(S_1^{(r)} = i \\mid \\mathbf{o}^{(r)})\n",
    "   = \\frac{1}{R} \\sum_{r=1}^R \\gamma^{(r)}_{i 1}.\n",
    "   $$\n",
    "\n",
    "2. **Transition probabilities**:\n",
    "   $$\n",
    "   a_{i j} \\leftarrow\n",
    "   \\frac\n",
    "     {\\sum_{r=1}^R \\sum_{t=1}^{T_r - 1} \\xi^{(r)}_{i j, t}}\n",
    "     {\\sum_{r=1}^R \\sum_{t=1}^{T_r - 1} \\gamma^{(r)}_{i t}}.\n",
    "   $$\n",
    "\n",
    "3. **Emission probabilities**:\n",
    "   $$\n",
    "   b_{i k} \\leftarrow\n",
    "   \\frac\n",
    "     {\\sum_{r=1}^R \\sum_{t=1}^{T_r} \\mathbf{1}\\{o^{(r)}_t = k\\} \\, \\gamma^{(r)}_{i t}}\n",
    "     {\\sum_{r=1}^R \\sum_{t=1}^{T_r} \\gamma^{(r)}_{i t}}.\n",
    "   $$\n",
    "\n",
    "Then repeat E-step and M-step until convergence (or until hitting a max iteration cap). The `MultinomialHMM.fit()` call is performing this EM loop internally.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a5d0caf",
   "metadata": {},
   "source": [
    "## 8. Identification / Authentication with Multiple User Models\n",
    "\n",
    "We now have **one HMM per user**:\n",
    "\n",
    "$$\n",
    "\\theta^{(u)} = (\\boldsymbol{\\pi}^{(u)}, A^{(u)}, B^{(u)}),\\quad u \\in \\mathcal{U}.\n",
    "$$\n",
    "\n",
    "Given a new keystroke sequence $\\mathbf{o}$ (timings for one password repetition), we compute:\n",
    "\n",
    "$$\n",
    "\\log P(\\mathbf{o} \\mid \\theta^{(u)}) \\quad \\text{for each user } u.\n",
    "$$\n",
    "\n",
    "If we assume a uniform prior over users, the **MAP estimate** of the user is:\n",
    "\n",
    "$$\n",
    "\\hat{u} = \\arg\\max_{u \\in \\mathcal{U}} \\log P(\\mathbf{o} \\mid \\theta^{(u)}).\n",
    "$$\n",
    "\n",
    "This is exactly what the evaluation code does:\n",
    "\n",
    "1. Convert the raw timings for that repetition into a discretized sequence $\\mathbf{o}$.\n",
    "2. For each user’s trained `MultinomialHMM`, call `score()` to compute the log-likelihood.\n",
    "3. Pick the user with the highest log-likelihood as the predicted identity.\n",
    "\n",
    "If we do this on a test set of repetitions, we can measure **identification accuracy**:\n",
    "\n",
    "$$\n",
    "\\text{Accuracy} = \\frac{\\# \\text{correctly identified repetitions}}{\\# \\text{total test repetitions}}.\n",
    "$$\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94fd0118",
   "metadata": {},
   "source": [
    "## 9. Intuition: What the HMM Learns About Typing\n",
    "\n",
    "- The hidden states $S_t$ capture **latent typing modes**:\n",
    "  - e.g. “smooth flow”, “small hesitation”, “big hesitation / correction”.\n",
    "- The transition matrix $A$ captures how a user tends to **move between modes** as they type:\n",
    "  - e.g. staying mostly in a steady mode, occasionally jumping to a “slow” mode at certain keys.\n",
    "- The emission matrix $B$ captures, for each mode, the **distribution over timing bins**:\n",
    "  - e.g. in one state, most timing bins are low (fast); in another, bins are higher (slow).\n",
    "\n",
    "Because these patterns are **user-specific**, the combination of $(\\boldsymbol{\\pi}, A, B)$ becomes a kind of **probabilistic signature** of that user’s typing behavior. When a new sequence of timings is more likely under one user’s HMM than others, the model uses that to **verify or identify** the user.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "23ba983b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_subject_subset(\n",
    "    subject_to_sequences: Dict[str, List[np.ndarray]],\n",
    "    num_users: int | None = None,\n",
    "    random_state: int = 0,\n",
    ") -> Dict[str, List[np.ndarray]]:\n",
    "    \"\"\"\n",
    "    Optionally restrict to a random subset of subjects.\n",
    "\n",
    "    - If num_users is None or >= total subjects, returns the original dict.\n",
    "    - Otherwise, randomly sample 'num_users' subjects and keep only those.\n",
    "    \"\"\"\n",
    "    if num_users is None:\n",
    "        return subject_to_sequences\n",
    "\n",
    "    subjects = list(subject_to_sequences.keys())\n",
    "    if num_users >= len(subjects):\n",
    "        return subject_to_sequences\n",
    "\n",
    "    rng = np.random.default_rng(random_state)\n",
    "    chosen = rng.choice(subjects, size=num_users, replace=False)\n",
    "\n",
    "    chosen = set(chosen)\n",
    "    return {s: seqs for s, seqs in subject_to_sequences.items() if s in chosen}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "12f04253",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_split_sequences(\n",
    "    subject_to_sequences: Dict[str, List[np.ndarray]],\n",
    "    train_frac: float = 0.7,\n",
    "    random_state: int = 0,\n",
    ") -> Tuple[\n",
    "    Dict[str, List[np.ndarray]],\n",
    "    Dict[str, List[np.ndarray]]\n",
    "]:\n",
    "    rng = np.random.default_rng(random_state)\n",
    "\n",
    "    train_dict: Dict[str, List[np.ndarray]] = {}\n",
    "    test_dict: Dict[str, List[np.ndarray]] = {}\n",
    "\n",
    "    for subj, seqs in subject_to_sequences.items():\n",
    "        seqs = list(seqs)\n",
    "        rng.shuffle(seqs)\n",
    "        n_train = int(len(seqs) * train_frac)\n",
    "        train_dict[subj] = seqs[:n_train]\n",
    "        test_dict[subj] = seqs[n_train:]\n",
    "\n",
    "    return train_dict, test_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "08999c3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_user_models_discrete_hmm(\n",
    "    train_sequences: Dict[str, List[np.ndarray]],\n",
    "    n_states: int,\n",
    "    n_obs: int,\n",
    "    n_iters: int = 20,\n",
    "    tol: float = 1e-3,\n",
    "    random_state: int = 0,\n",
    ") -> Dict[str, MultinomialHMM]:\n",
    "    \"\"\"\n",
    "    Train one MultinomialHMM per subject using hmmlearn, with one-hot\n",
    "    observations (n_trials=1 -> categorical case).\n",
    "    \"\"\"\n",
    "    models: Dict[str, MultinomialHMM] = {}\n",
    "\n",
    "    for subj, seqs in train_sequences.items():\n",
    "        if len(seqs) == 0:\n",
    "            continue\n",
    "\n",
    "        # Convert this subject's block sequences into one big one-hot matrix\n",
    "        X, lengths = sequences_to_one_hot(seqs, n_obs=n_obs)\n",
    "\n",
    "        model = MultinomialHMM(\n",
    "            n_components=n_states,\n",
    "            n_trials=1,        # categorical case\n",
    "            n_iter=n_iters,\n",
    "            tol=tol,\n",
    "            random_state=random_state,\n",
    "            verbose=False,\n",
    "        )\n",
    "\n",
    "        # X: (sum_T, n_obs), lengths: list of block lengths\n",
    "        model.fit(X, lengths)\n",
    "        models[subj] = model\n",
    "\n",
    "    return models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "77a6fdaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_subject_discrete_hmm(\n",
    "    models: Dict[str, MultinomialHMM],\n",
    "    seq: np.ndarray,\n",
    "    n_obs: int,\n",
    ") -> Tuple[str, Dict[str, float]]:\n",
    "    \"\"\"\n",
    "    Predict subject by maximum log-likelihood for a single repetition sequence.\n",
    "\n",
    "    seq: array of shape (T,) with symbols in [0, n_obs-1], T≈31\n",
    "    \"\"\"\n",
    "    seq = np.asarray(seq, dtype=int)\n",
    "    X = np.eye(n_obs, dtype=float)[seq]  # (T, n_obs)\n",
    "\n",
    "    scores: Dict[str, float] = {}\n",
    "    for subj, model in models.items():\n",
    "        scores[subj] = float(model.score(X))\n",
    "\n",
    "    best_subj = max(scores, key=scores.get)\n",
    "    return best_subj, scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "51300c74",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_identification_discrete_hmm(\n",
    "    models: Dict[str, MultinomialHMM],\n",
    "    test_sequences: Dict[str, List[np.ndarray]],\n",
    "    n_obs: int,\n",
    ") -> float:\n",
    "    \"\"\"\n",
    "    Identification accuracy: given a single repetition, can we identify the subject?\n",
    "    \"\"\"\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    for true_subj, seqs in test_sequences.items():\n",
    "        for seq in seqs:\n",
    "            pred_subj, _ = predict_subject_discrete_hmm(models, seq, n_obs)\n",
    "            total += 1\n",
    "            if pred_subj == true_subj:\n",
    "                correct += 1\n",
    "\n",
    "    return correct / total if total > 0 else 0.0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "8bf63494",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/envs/datasci-sec/lib/python3.11/site-packages/sklearn/preprocessing/_discretization.py:296: FutureWarning: The current default behavior, quantile_method='linear', will be changed to quantile_method='averaged_inverted_cdf' in scikit-learn version 1.9 to naturally support sample weight equivalence properties by default. Pass quantile_method='averaged_inverted_cdf' explicitly to silence this warning.\n",
      "  warnings.warn(\n",
      "MultinomialHMM has undergone major changes. The previous version was implementing a CategoricalHMM (a special case of MultinomialHMM). This new implementation follows the standard definition for a Multinomial distribution (e.g. as in https://en.wikipedia.org/wiki/Multinomial_distribution). See these issues for details:\n",
      "https://github.com/hmmlearn/hmmlearn/issues/335\n",
      "https://github.com/hmmlearn/hmmlearn/issues/340\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total subjects available: 51\n",
      "Subjects actually used: ['s018', 's020', 's031', 's036', 's046']\n",
      "Number of users used: 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "MultinomialHMM has undergone major changes. The previous version was implementing a CategoricalHMM (a special case of MultinomialHMM). This new implementation follows the standard definition for a Multinomial distribution (e.g. as in https://en.wikipedia.org/wiki/Multinomial_distribution). See these issues for details:\n",
      "https://github.com/hmmlearn/hmmlearn/issues/335\n",
      "https://github.com/hmmlearn/hmmlearn/issues/340\n",
      "MultinomialHMM has undergone major changes. The previous version was implementing a CategoricalHMM (a special case of MultinomialHMM). This new implementation follows the standard definition for a Multinomial distribution (e.g. as in https://en.wikipedia.org/wiki/Multinomial_distribution). See these issues for details:\n",
      "https://github.com/hmmlearn/hmmlearn/issues/335\n",
      "https://github.com/hmmlearn/hmmlearn/issues/340\n",
      "MultinomialHMM has undergone major changes. The previous version was implementing a CategoricalHMM (a special case of MultinomialHMM). This new implementation follows the standard definition for a Multinomial distribution (e.g. as in https://en.wikipedia.org/wiki/Multinomial_distribution). See these issues for details:\n",
      "https://github.com/hmmlearn/hmmlearn/issues/335\n",
      "https://github.com/hmmlearn/hmmlearn/issues/340\n",
      "MultinomialHMM has undergone major changes. The previous version was implementing a CategoricalHMM (a special case of MultinomialHMM). This new implementation follows the standard definition for a Multinomial distribution (e.g. as in https://en.wikipedia.org/wiki/Multinomial_distribution). See these issues for details:\n",
      "https://github.com/hmmlearn/hmmlearn/issues/335\n",
      "https://github.com/hmmlearn/hmmlearn/issues/340\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Per-repetition identification accuracy (N=5 users): 0.745\n"
     ]
    }
   ],
   "source": [
    "# 1) Build discrete sequences (one repetition = one sequence of length 31)\n",
    "subject_to_sequences, n_obs, discretizer = build_sequences_discrete(\n",
    "    keystroke_data,\n",
    "    n_bins=10,\n",
    "    min_reps_per_subject=20,\n",
    ")\n",
    "\n",
    "print(\"Total subjects available:\", len(subject_to_sequences))\n",
    "\n",
    "\n",
    "NUM_USERS = 5   # for example, only use 5 users\n",
    "subject_to_sequences = select_subject_subset(\n",
    "    subject_to_sequences,\n",
    "    num_users=NUM_USERS,\n",
    "    random_state=0,\n",
    ")\n",
    "\n",
    "print(\"Subjects actually used:\", list(subject_to_sequences.keys()))\n",
    "print(\"Number of users used:\", len(subject_to_sequences))\n",
    "\n",
    "# 2) Train/test split over repetitions (only for those users)\n",
    "train_seq, test_seq = train_test_split_sequences(\n",
    "    subject_to_sequences,\n",
    "    train_frac=0.7,\n",
    "    random_state=0,\n",
    ")\n",
    "\n",
    "# 3) Train one HMM per subject\n",
    "models = train_user_models_discrete_hmm(\n",
    "    train_seq,\n",
    "    n_states=5,\n",
    "    n_obs=n_obs,\n",
    "    n_iters=30,\n",
    "    tol=1e-3,\n",
    "    random_state=0,\n",
    ")\n",
    "\n",
    "# 4) Evaluate per-repetition identification accuracy\n",
    "acc = evaluate_identification_discrete_hmm(models, test_seq, n_obs=n_obs)\n",
    "print(f\"Per-repetition identification accuracy (N={len(models)} users): {acc:.3f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "datasci-sec",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
